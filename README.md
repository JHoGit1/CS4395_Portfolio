# CS4395_Portfolio
This is a portfolio for the CS4395 Human Language Technologies (Natural Language Processing) class to showcase the different algorithms and techniques that are utilized to perform analysis on text. After taking this course I have gained valuble insight into the topic of NLP. Before taking this course, I did not realize how much NLP was utilized to run various applications and programs on our computers and even in our daily lives. From chatbots to word autocorrectors and live caption translation, NLP is a growing field that will only expand as our technology grows. It'll be astonishing to compare NLP progress years from now versus what we have established currently. We may believe that what we knew before was so primitive!

Of all the components here, if there is one I'd potentially would like to continue one day, it would be the recipe chatbot. Dialogflow is such a complex system that we barely scratched the surface of it when making our recipe chatbot. I hope with more time after graduation, we'd be able to pursue working on it more as a personal project. 

I think the one thing that not only I would see, but the whole world would see grow is ChatGPT. Its exponential growth since the beginning of the semester has been astonishing, and has brought up new discussions about AI in our society. I believe we'll be hearing about ChatGPT for a very long time.

## Component 0
This is an introduction to what NLP is and my personal short view NLP.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%200/C0_Jonathan_Ho_Overview_of_NLP.pdf) to view the short document.

## Component 1
The following portfolio component utilizes Python to create a simple script that can take in and process data from a .csv file.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%201/jqh200000_A1.py) to see the python script.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%201/C1_Jonathan_Ho_Python_Overview.pdf) for a small overview about the script.

## Component 2
In this component, a Python script was created to be able to preprocess and tokenize a text file, find the 50 most common used nouns, and pick one word from the common nouns to use for a guessing game.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%202/jqh200000_A2.py) to see the python script for the guessing game.

## Component 3
Google Colab was used to explore the WordNet package from NLTK. Elements that were explored were synsets, word similarity checks, and word sentiment values.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%203/C3_Jonathan_Ho_WordNet.pdf) to see the notebook using WordNet.

## Component 4
Exploration of Ngrams were conducted by utilizing Python code. The code is split into two programs. The first program took in texts to create unigram and bigram dictionaries and pickled them. Then, the second program would unpickle the dictionaries to interpret a test file with lines of languages varying between English, French, and Italian. The language model would determine the probabilities of each language for each line and output to a text file what language it thinks that test line is. An overview narrative was then written.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%204/jqh200000_A4_P1.py) for the code of the first program.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%204/jqh200000_A4_P2.py) for the code of the second program.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%204/jqh200000_A4_Narrative.pdf) for the narrative on ngrams.

## Component 5
Different sentence parsing techniques were viewed in this assignment. Through AllenNLP, PSG, dependancy, and SRL parsing were carried out on a complex sentence.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%205/jqh200000_A5_Writeup.pdf) to view the write up on the different parsers.

## Component 6
A web crawler was created to look for links with a term, find the text within the links, clean up the links, and find their term frequencies. Ten significant terms were chosen across the links, and a knowledge base was formulated using a simple Python dictionary.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%206/jqh200000_A6.py) to view the code for the web crawler.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%206/jqh200000_A6_Knowledge_Base_Writeup.pdf) to view the short write up of the knowledge base.

## Component 7
Naive Bayes, Logistic Regression, and Neural Networks were used to create models to predict text classification. 

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%207/jqh200000_A7.pdf) to view the notebok for the text classification.

## Component 8
An peer-reviewed paper was chosen from the ACL Anthology website to read and summarize. The link for the specific paper that was read is [here](https://aclanthology.org/2022.acl-long.27/). 

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%208/jqh200000_A8.pdf) to see the summary of the ACL paper chosen.

## Component 9

A simple Recipe Chatbot was created using Google's DialogFlow ES. The project was done in collaboration with David Park.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/tree/main/CS4395_Portfolio/Component%209) to view the source code and static knowledge base of the chatbot.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%209/Chatbot_Report.pdf) to view the writeup of the chatbot.

## Component 10
Different deep learning models were created to try and predict whether or not text was spam. This includes a sequential model, RNN, CNN, LSTM, and GRU as well as a GloVe embedding.

Click [here](https://github.com/JHoGit1/UTD_CS_Portfolio/blob/main/CS4395_Portfolio/Component%2010/jqh200000_A10.pdf) to see the overview.